# k-means_clustering_projects
K-Means Clustering: From Scratch Implementation This project involves building the K-Means clustering algorithm from the ground up using NumPy. By avoiding high-level libraries like scikit-learn for the core logic, you will gain a deep functional understanding of iterative optimization.

Initialization SensitivityBased on the results, you will likely notice that the Standard Deviation for $K=2$ or $K=4$ might be higher than for $K=3$. This is because when $K$ doesn't match the "natural" number of clusters, the algorithm is more sensitive to where the centroids startâ€”sometimes it splits a natural cluster in half, and other times it groups two distinct clusters together, resulting in different inertia values.Justification for the 'Optimal' KThe Elbow Method logic suggests we look for the point where the decrease in inertia slows down significantly.From K=2 to K=3: You will see a massive drop in inertia because we are moving from "underfitting" to "matching" the data structure.From K=3 to K=4: You will see a much smaller decrease (diminishing returns) because the fourth cluster is essentially just splitting a perfectly fine existing cluster.Conclusion: The optimal $K$ is 3, as it provides the most significant "elbow" in the inertia curve and matches the synthetic generation parameters
